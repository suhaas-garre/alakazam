# AUTOGENERATED! DO NOT EDIT! File to edit: mini_shazam.ipynb.

# %% auto 0
__all__ = ['read_audio_file', 'remove_dc', 'apply_max_filter', 'get_track_id', 'find_point_relationships', 'create_song_df',
           'generate_fingerprint', 'add_song_to_db', 'identify_song']

# %% mini_shazam.ipynb 7
import os
from pathlib import Path
import time
import numpy as np
import pandas as pd
import scipy
import matplotlib.pyplot as plt
# from pydub import AudioSegment
from scipy.io import wavfile
from tempfile import mktemp
import scipy.spatial as spatial


# %% mini_shazam.ipynb 27
# read music file after identifying file type 
# only works with .wav for now though lol
def read_audio_file(filepath):
    file_type = Path(filepath).suffix

    if file_type == '.wav':
        sampling_rate, data = wavfile.read(filepath)  # read wav file
        data = np.mean(data, axis=1) # average the two stereo channels
    else:
        raise Exception('sorry, other file types not supported yet lol')
        
    return sampling_rate, data

# fn to remove dc offset from signal
def remove_dc(arr): return arr - np.mean(arr)

# maximum filtering to identify peaks
# input: times, frequencies, amplitudes of signal from spectrogram
# output: list of points that correspond to unchanged peaks
def apply_max_filter(ts, freqs, amps):
    # time_interval and freq_interval calculated so that we can get even spacing throughout frequencies and time
    time_interval = len(ts) / np.max(ts)
    freq_interval = len(freqs) / 10

    # performing maximum filtering
    max_filter = scipy.ndimage.maximum_filter(amps, size=(freq_interval, time_interval))

    # identify which values are unchanged after doing filtering
    unchanged = max_filter == amps # unchanged = boolean matrix
    
    # get the coordinates of unchanged peaks
    # NOTE: THESE ARE THE INDICES, NOT THE VALUES THEMSELVES
    peak_freqs, peak_ts = np.where(unchanged)

    # make two column array to hold peak frequencies and times
    peak_ts_freqs = np.zeros((len(peak_freqs),2))
    
    # replace indices with corresponding times and frequencies
    peak_ts_freqs[:,0] = ts[peak_ts.astype(int)]
    peak_ts_freqs[:,1] = freqs[peak_freqs.astype(int)]
    
    # sort by time column (just seems like it'll make things easier?)
    peak_ts_freqs = peak_ts_freqs[peak_ts_freqs[:,0].argsort()]

    return peak_ts_freqs

# get track id from filepath
def get_track_id(filepath): return Path(filepath).stem

# identify all point relationships for robust constellation
def find_point_relationships(list_of_points, relationship_tree, distance):
    # initialize list to hold all relationship arrays so we can process everything at once
    temp_list = []
    
    # iterate through each point, get all points within some distance (will need to play around with distance value), filter only
    # points that are later in time, create hash, add to pandas dataframe pairwise
    num_points = len(list_of_points)
    for i in range(num_points):
        # get current point
        cur = list_of_points[i]
        
        # get all values within distance 500 of point 
        nearby = list_of_points[relationship_tree.query_ball_point(cur, distance)]
        
        # filter only points that are later in time
        after = nearby[nearby[:,0] > cur[0]]
        point_arr = np.zeros((len(after),8)) # one column for each df column, one row for each point
        
        point_arr[:,0] = cur[1] # point 1 frequencies
        point_arr[:,1] = after[:,1] # frequencies of all point 2s
        
        point_arr[:,2] = cur[0] # point 1 times
        point_arr[:,3] = after[:,0] # times of all point 2s
        
        temp_list.append(point_arr) # appending to the list is most efficient, process all at the end
    
    # concatenate all arrays in the list along the rows (axis=0)
    point_relationships = np.vstack(temp_list)
    
    return point_relationships

# fn to create song_df from array of point relationships
def create_song_df(point_relationships):
    # column names for dataframe
    columns=['f1', 'f2', 't1', 't2', 't_delta', 'hash_tuple', 'hash', 'track_id']

    # create df with all point relationships
    song_df = pd.DataFrame(data=point_relationships, columns=columns)
    
    # round each column to corresponding decimal places
    song_df = song_df.round({'f1': 3, 'f2': 3, 't1': 3, 't2': 3})

    # calculate hashes from time difference and frequencies
    song_df['t_delta'] = song_df['t2'] - song_df['t1']
    song_df['hash_tuple'] = list(zip(np.round(song_df['f1'], decimals=3), 
                                     np.round(song_df['f2'], decimals=3), 
                                     np.round(song_df['t_delta'], decimals=3)
                                    )
                                )
    song_df['hash'] = song_df['hash_tuple'].apply(hash)
    
    return song_df 
    
# fingerprint song and add to csv file
def generate_fingerprint(wav_filepath):
    # get track id for song identification
    track_id = get_track_id(wav_filepath)

    # read music file data
    sample_rate, samples = read_audio_file(wav_filepath)
    samples = remove_dc(samples) # remove DC offset

    # generate spectrogram, get frequencies, times, amplitudes
    freqs, ts, amps = scipy.signal.spectrogram(samples, sample_rate)

    # apply max filter and get list of points that correspond to unchanged peaks
    peak_ts_freqs = apply_max_filter(ts, freqs, amps)
    
    # create KDTree of all points so that we can identify neighborhoods (will be needed for 
    # robust constellations)
    kd_tree = spatial.KDTree(peak_ts_freqs)

    # find all point relationships within specified distance
    point_relationships = find_point_relationships(peak_ts_freqs, kd_tree, 200)

    # create song_df with above relationships
    song_df = create_song_df(point_relationships)

    # set track_id
    song_df['track_id'] = track_id

    # keep only the columns that we need for the csv db file
    columns_to_export = song_df[['t1', 'hash', 'track_id']].copy()

    return columns_to_export

# append to the master song hash db file
def add_song_to_db(song_filepath, db_filepath=db_filepath):
    song_fingerprint = generate_fingerprint(song_filepath) # generate fingerprint for new song
    db = pd.read_csv(db_filepath, index_col=0) # load db from file
    db = pd.concat([db, song_fingerprint], ignore_index=True) # add song df to db file
    db.to_csv(db_filepath) # save db again to same file
    return db

# %% mini_shazam.ipynb 33
def identify_song(snippet_filepath, db_filepath=db_filepath):
    # load up the db brother
    db = pd.read_csv(db_filepath, index_col=0)
    
    # generate fingerprint in the same way as before
    snippet_df = generate_fingerprint(snippet_filepath)

    # identify rows in db_file that correspond to hashes in snippet_df
    filtered_db = db[db['hash'].isin(snippet_df['hash'])]

    # Merge the dataframes using the 'hash' column
    merged_df = pd.merge(filtered_db, snippet_df, on='hash', suffixes=('_db', '_snippet'))

    # calculate time offset of our snippet from the original file
    merged_df['offset'] = merged_df['t1_db'] - merged_df['t1_snippet']

    # find most common time offset for matching hashes
    most_common_offset = merged_df[merged_df['offset'] == merged_df['offset'].mode()[0]]

    # identify most common track id from these common time offsets
    most_likely_song = most_common_offset['track_id_db'].mode()[0]

    return most_likely_song
